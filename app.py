import torch
from transformers import AutoConfig, AutoModel, AutoTokenizer
import gradio as gr

#These texts support HTML and Markdown
title = "ğŸ‘¸ChatGLM2 with Rua"
description = "æˆ‘æ‹¥æœ‰Rua17å²è‡³20å²çš„è®°å¿†ã€‚<b>æ³¨æ„</b>ï¼Œæ¯”èµ·ChatGPTç±»é¡¹ç›®ï¼Œæˆ‘æ— æ³•å¸®ä½ å®Œæˆä»»åŠ¡ï¼Œç”šè‡³æœ‰æ—¶å›ç­”ä¸å‡ºä½ çš„ç®€å•é—®é¢˜ã€‚æˆ‘èƒ½åšå‡ºçš„åé¦ˆæ›´åƒæ˜¯ï¼šåœ¨ä½ æä¾›çš„è¯­å¢ƒä¸‹ï¼ŒçœŸæ­£çš„Ruaä¼šè¯´ä»€ä¹ˆï¼Ÿ<br>è®­ç»ƒæ‰€ä½¿ç”¨çš„æ•°æ®å…¨éƒ¨æ¥æºäºRuaè‡ªå·±å‘å‡ºçš„æ¶ˆæ¯ï¼Œä¿¡æ¯ç»è¿‡æ¸…æ´—å’Œè„±æ•ã€‚ä»»ä½•æ¶‰åŠä¸ªäººä¿¡æ¯çš„å›ç­”å‡ä¸ºæ¨¡å‹è‡ªå·±ç¼–çš„ï¼Œ<b>åˆ«ä¿¡</b>ã€‚<br>åŠ è½½å¯èƒ½ä¼šæ¯”è¾ƒæ…¢ï¼Œç‚¹å‡»ä¸‹é¢çš„ä¾‹å­å¯èƒ½éœ€è¦å‡ ç§’æ‰ä¼šæ˜¾ç¤ºåœ¨inputä¸­ï¼Œç‚¹ä¸€æ¬¡å°±å¯ä»¥å•¦ä¸è¦ä¸€ç›´æˆ³ã€‚<br>ä¸€ä¸ªå¯¹è¯çš„åŠ è½½æ—¶é—´éœ€è¦<b>1~2åˆ†é’Ÿ</b>ï¼Œæ˜¯ç¡¬ä»¶åŸå› ï¼Œè¯·è€å¿ƒç­‰å¾…æ‹œæ‰˜å•¦ã€‚å¦‚æœä½ æ„¿æ„ä¹Ÿå¯ä»¥ç»™Ruaå……é’±è®©å¥¹å‡çº§ä»“åº“é…ç½®ã€‚"
examples = [["ä½ æ™šä¸Šæƒ³åƒä»€ä¹ˆ"],["ä½ åœ¨å¹²ä»€ä¹ˆ"],["ä»€ä¹ˆæ—¶å€™å‡ºå»ç©"]] #Those options can be clicked directly to input on the web page for players 

#Below are the same as testing on kaggle
model_path = "THUDM/chatglm2-6b-int4"
prefix_state_dict = "./{your-pytorch-model}.bin" #make sure the ./ is added!!!

tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
config = AutoConfig.from_pretrained(model_path, trust_remote_code=True, pre_seq_len=128)
model = AutoModel.from_pretrained(model_path, config=config, trust_remote_code=True)

prefix_state_dict = torch.load(prefix_state_dict,map_location=torch.device('cpu'))
new_prefix_state_dict = {}
for k, v in prefix_state_dict.items():
    if k.startswith("transformer.prefix_encoder."):
        new_prefix_state_dict[k[len("transformer.prefix_encoder."):]] = v
model.transformer.prefix_encoder.load_state_dict(new_prefix_state_dict)

model = model.quantize(4)
model = model.float()
model.transformer.prefix_encoder.float()

model = model.eval()


def predict(input, state=[]):
    response, dialog = model.chat(tokenizer,input, history=[])
    print(response, dialog)
    history = state + dialog #this is to ensure the chat history will be displayed
    return history, history


gr.Interface(
    fn=predict,
    title=title,
    description=description,
    examples=examples,
    inputs=["text", "state"],
    outputs=["chatbot","state"], # adding state is to ensure the history can be passed to the next round
    theme="ParityError/Anime", #choose any theme you want on https://huggingface.co/spaces/gradio/theme-gallery
).launch()